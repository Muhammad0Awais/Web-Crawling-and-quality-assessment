{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "2021S-01 Crawler.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7C_XyW0x1XS"
      },
      "source": [
        "# Crawler\n",
        "## 1. Download and persist #\n",
        "Please complete a code for `load()`, `download()` and `persist()` methods of `Document` class. What they do:\n",
        "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
        "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
        "- `load()` method loads data from hard drive. Returns `True` for success.\n",
        "\n",
        "Tests checks that your code somehow works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIXOVAsvx1XZ"
      },
      "source": [
        "import requests\n",
        "from urllib.parse import quote\n",
        "\n",
        "class Document:\n",
        "    \n",
        "    def __init__(self, url):\n",
        "        self.url = url\n",
        "\n",
        "        \n",
        "    def get(self):\n",
        "        if not self.load():\n",
        "            if not self.download():\n",
        "                raise FileNotFoundError(self.url)\n",
        "            else:\n",
        "                self.persist()\n",
        "    \n",
        "    def download(self):\n",
        "        r = requests.get(self.url, allow_redirects=True) # Getting contents\n",
        "        if r: \n",
        "          self.content = r.content\n",
        "          return True\n",
        "        #TODO download self.url content, store it in self.content and return True in case of success\n",
        "        return False\n",
        "    \n",
        "    def persist(self):\n",
        "        #TODO write document content to hard drive\n",
        "        # url is attached with a file name for uniqueness\n",
        "\n",
        "        url = quote(self.url, safe = \"\")\n",
        "        open(\"nfile_\" + url, 'wb').write(self.content)\n",
        "            \n",
        "    def load(self):\n",
        "        try:\n",
        "          url = quote(self.url, safe = \"\")\n",
        "          f = open(\"nfile_\" + url, \"rb\")\n",
        "          self.content = f.read()\n",
        "          return True\n",
        "        except FileNotFoundError:\n",
        "          return False\n",
        "          #TODO load content from hard drive, store it in self.content and return True in case of success\n",
        "        return False"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l838r1wzx1Xa"
      },
      "source": [
        "### 1.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0UDRHhox1Xa"
      },
      "source": [
        "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
        "\n",
        "doc.get()\n",
        "assert doc.content, \"Document download failed\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
        "\n",
        "doc.get()\n",
        "assert doc.load(), \"Load should return true for saved document\"\n",
        "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoNxWMbAx1Xb"
      },
      "source": [
        "## 2. Parse HTML ##\n",
        "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. You should initialize:\n",
        "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
        "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
        "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-KCYqY5Rx1Xb"
      },
      "source": [
        "from bs4 import BeautifulSoup, SoupStrainer\n",
        "from bs4.element import Comment\n",
        "import urllib.parse\n",
        "from urllib.parse import urljoin\n",
        "import re\n",
        "import json\n",
        "\n",
        "\n",
        "class HtmlDocument(Document):\n",
        "\n",
        "    def parse(self):\n",
        "        ancrs = []\n",
        "        images = []\n",
        "\n",
        "        soup = BeautifulSoup(self.content, 'html.parser')\n",
        "        \n",
        "        # Finding links with anchor tags in the data\n",
        "\n",
        "        ancrs = soup.find_all('a')\n",
        "        ancrs = [(anc.contents[0], urljoin(self.url, anc.get('href',''))) for anc in ancrs]\n",
        "        \n",
        "        # Finding images with image tags in the data\n",
        "\n",
        "        imgs = soup.find_all('img')\n",
        "        imgs = [urljoin(self.url, img.get('src','')) for img in imgs]\n",
        "        \n",
        "        # Finding the text from the data.\n",
        "\n",
        "        txts = soup.find_all(text = True)\n",
        "        text = ''.join(str(e) for e in txts)\n",
        "        \n",
        "        # print(len(imgs))\n",
        "\n",
        "        #TODO exctact plain text, images and links from the document\n",
        "        self.anchors = ancrs\n",
        "        self.images = imgs\n",
        "        self.text = text"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYvFv4Qqx1Xc"
      },
      "source": [
        "### 2.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbYiAIhxx1Xc"
      },
      "source": [
        "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
        "doc.get()\n",
        "doc.parse()\n",
        "\n",
        "assert \"тестирующий сервер codetest\" in doc.text, \"Error parsing text\"\n",
        "assert \"http://sprotasov.ru/images/phone.png\" in doc.images, \"Error parsing images\"\n",
        "assert any(p[1] == \"http://university.innopolis.ru/\" for p in doc.anchors), \"Error parsing links\""
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7tondV3x1Xd"
      },
      "source": [
        "## 3. Document analysis ##\n",
        "Complete the code for `HtmlDocumentTextData` class. Implement word and sentence splitting (use any method you can propose). Your `get_word_stats()` method should return `Counter` object. Don't forget to lowercase your words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQe7JfPex1Xd"
      },
      "source": [
        "from collections import Counter\n",
        "import wordcounter\n",
        "\n",
        "\n",
        "class HtmlDocumentTextData:\n",
        "    \n",
        "    def __init__(self, url):\n",
        "        self.doc = HtmlDocument(url)\n",
        "        self.doc.get()\n",
        "        self.doc.parse()\n",
        "    \n",
        "    def get_sentences(self):\n",
        "        #TODO*: implement sentence parser\n",
        "        # Spliting the text to get the sentences\n",
        "        result = self.doc.text.splitlines()\n",
        "        return result\n",
        "    \n",
        "    def get_word_stats(self):\n",
        "        #TODO return Counter object of the document, containing mapping {`word` -> count_in_doc}\n",
        "        \n",
        "        # First we got sentences\n",
        "        sentences =  self.get_sentences()\n",
        "\n",
        "        # Spliting the words\n",
        "\n",
        "        wordfreq = []\n",
        "        for sentence in sentences:\n",
        "            wordfreq.extend(re.split(' +', sentence.lower()))\n",
        "        \n",
        "        return Counter(wordfreq)"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ1V2rv2x1Xe"
      },
      "source": [
        "### 3.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtpmDf3Lx1Xe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2ce7d8-3758-4fca-a8a6-26b4fa91bfb7"
      },
      "source": [
        "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
        "\n",
        "print(doc.get_word_stats().most_common(10))\n",
        "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис sould be among most common'"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[('', 804), ('и', 46), ('в', 30), ('иннополис', 21), ('=', 14), ('на', 13), ('по', 13), ('университет', 11), ('университета', 10), ('лаборатория', 10)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CIpLTFvx1Xf"
      },
      "source": [
        "## 4. Crawling ##\n",
        "\n",
        "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBrMoCjpx1Xf"
      },
      "source": [
        "from queue import Queue\n",
        "\n",
        "class Crawler:\n",
        "    \n",
        "    def crawl_generator(self, source, depth=1):\n",
        "        #TODO return real crawling results. Don't forget to process failures\n",
        "        q = Queue()\n",
        "        q.put(source)\n",
        "\n",
        "        for i in range(depth):\n",
        "          url = q.get()\n",
        "          content = HtmlDocumentTextData(url)\n",
        "\n",
        "          for (text, url) in content.doc.anchors:\n",
        "            q.put(url)\n",
        "          \n",
        "          yield content"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpSFPYCBx1Xg"
      },
      "source": [
        "### 4.1. Tests ###"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGuyqohNx1Xg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2ffe500-ad54-44b0-cea9-71433128f13f"
      },
      "source": [
        "crawler = Crawler()\n",
        "counter = Counter()\n",
        "\n",
        "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
        "    print(c.doc.url)\n",
        "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
        "        print(\"Skipping\", c.doc.url)\n",
        "        continue\n",
        "    counter.update(c.get_word_stats())\n",
        "    print(len(counter), \"distinct word(s) so far\")\n",
        "    \n",
        "print(\"Done\")\n",
        "\n",
        "print(counter.most_common(20))\n",
        "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://innopolis.university/en/\n",
            "485 distinct word(s) so far\n",
            "http://old.innopolis.university/en/\n",
            "1049 distinct word(s) so far\n",
            "Done\n",
            "[('', 1768), ('the', 108), ('of', 89), ('and', 87), ('=', 43), ('innopolis', 40), ('in', 37), ('to', 37), ('for', 37), ('university', 33), ('will', 24), ('research', 24), ('a', 22), ('ул.', 22), ('new', 20), ('it', 20), ('education', 20), ('international', 19), ('programs', 16), ('institute', 16)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}