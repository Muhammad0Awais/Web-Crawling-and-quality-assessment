{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "colab": {
   "name": "2021S-01 Crawler.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7C_XyW0x1XS"
   },
   "source": [
    "# Crawler\n",
    "## 1. Download and persist #\n",
    "- for a given URL `download()` method downloads binary data and stores in `self.content`. It returns `True` for success, else `False`.\n",
    "- `persist()` method saves `self.content` somewhere in file system. We do it to avoid multiple downloads (for caching in other words).\n",
    "- `load()` method loads data from hard drive. Returns `True` for success.\n",
    "\n",
    "Tests checks that your code somehow works."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "lIXOVAsvx1XZ"
   },
   "source": [
    "import requests\n",
    "from urllib.parse import quote\n",
    "\n",
    "class Document:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "\n",
    "        \n",
    "    def get(self):\n",
    "        if not self.load():\n",
    "            if not self.download():\n",
    "                raise FileNotFoundError(self.url)\n",
    "            else:\n",
    "                self.persist()\n",
    "    \n",
    "    def download(self):\n",
    "        r = requests.get(self.url, allow_redirects=True) # Getting contents\n",
    "        if r: \n",
    "          self.content = r.content\n",
    "          return True\n",
    "        return False\n",
    "    \n",
    "    def persist(self):\n",
    "        #TODO write document content to hard drive\n",
    "        # url is attached with a file name for uniqueness\n",
    "\n",
    "        url = quote(self.url, safe = \"\")\n",
    "        open(\"nfile_\" + url, 'wb').write(self.content)\n",
    "            \n",
    "    def load(self):\n",
    "        try:\n",
    "          url = quote(self.url, safe = \"\")\n",
    "          f = open(\"nfile_\" + url, \"rb\")\n",
    "          self.content = f.read()\n",
    "          return True\n",
    "        except FileNotFoundError:\n",
    "          return False\n",
    "        return False"
   ],
   "execution_count": 106,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l838r1wzx1Xa"
   },
   "source": [
    "### 1.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "W0UDRHhox1Xa"
   },
   "source": [
    "doc = Document('http://sprotasov.ru/data/iu.txt')\n",
    "\n",
    "doc.get()\n",
    "assert doc.content, \"Document download failed\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document content error\"\n",
    "\n",
    "doc.get()\n",
    "assert doc.load(), \"Load should return true for saved document\"\n",
    "assert \"Code snippets, demos and labs for the course\" in str(doc.content), \"Document load from disk error\""
   ],
   "execution_count": 107,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MoNxWMbAx1Xb"
   },
   "source": [
    "## 2. Parse HTML ##\n",
    "`BeautifulSoap` library is a de facto standard to parse XML and HTML documents in python. Use it to complete `parse()` method that extracts document contents. initialization:\n",
    "- `self.anchors` list of tuples `('text', 'url')` met in a document. Be aware, there exist relative links (e.g. `../content/pic.jpg`). Use `urllib.parse.urljoin()` to fix this issue.\n",
    "- `self.images` list of images met in a document. Again, links can be relative to current page.\n",
    "- `self.text` should keep plain text of the document without scripts, tags, comments and so on. You can refer to [this stackoverflow answer](https://stackoverflow.com/a/1983219) for details."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-KCYqY5Rx1Xb",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "from bs4 import BeautifulSoup, SoupStrainer\n",
    "from bs4.element import Comment\n",
    "import urllib.parse\n",
    "from urllib.parse import urljoin\n",
    "import re\n",
    "import json\n",
    "\n",
    "\n",
    "class HtmlDocument(Document):\n",
    "\n",
    "    def parse(self):\n",
    "        ancrs = []\n",
    "        images = []\n",
    "\n",
    "        soup = BeautifulSoup(self.content, 'html.parser')\n",
    "        \n",
    "        # Finding links with anchor tags in the data\n",
    "\n",
    "        ancrs = soup.find_all('a')\n",
    "        ancrs = [(anc.contents[0], urljoin(self.url, anc.get('href',''))) for anc in ancrs]\n",
    "        \n",
    "        # Finding images with image tags in the data\n",
    "\n",
    "        imgs = soup.find_all('img')\n",
    "        imgs = [urljoin(self.url, img.get('src','')) for img in imgs]\n",
    "        \n",
    "        # Finding the text from the data.\n",
    "\n",
    "        txts = soup.find_all(text = True)\n",
    "        text = ''.join(str(e) for e in txts)\n",
    "        \n",
    "        # print(len(imgs))\n",
    "        self.anchors = ancrs\n",
    "        self.images = imgs\n",
    "        self.text = text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYvFv4Qqx1Xc"
   },
   "source": [
    "### 2.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mbYiAIhxx1Xc"
   },
   "source": [
    "doc = HtmlDocument(\"http://sprotasov.ru\")\n",
    "doc.get()\n",
    "doc.parse()\n",
    "\n",
    "assert \"тестирующий сервер codetest\" in doc.text, \"Error parsing text\"\n",
    "assert \"http://sprotasov.ru/images/phone.png\" in doc.images, \"Error parsing images\"\n",
    "assert any(p[1] == \"http://university.innopolis.ru/\" for p in doc.anchors), \"Error parsing links\""
   ],
   "execution_count": 109,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7tondV3x1Xd"
   },
   "source": [
    "## 3. Document analysis ##\n",
    "Implementing word and sentence splitting . `get_word_stats()` method returns `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mQe7JfPex1Xd"
   },
   "source": [
    "from collections import Counter\n",
    "import wordcounter\n",
    "\n",
    "\n",
    "class HtmlDocumentTextData:\n",
    "    \n",
    "    def __init__(self, url):\n",
    "        self.doc = HtmlDocument(url)\n",
    "        self.doc.get()\n",
    "        self.doc.parse()\n",
    "    \n",
    "    def get_sentences(self):\n",
    "        # Spliting the text to get the sentences\n",
    "        result = self.doc.text.splitlines()\n",
    "        return result\n",
    "    \n",
    "    def get_word_stats(self):\n",
    "        \n",
    "        # First we got sentences\n",
    "        sentences =  self.get_sentences()\n",
    "\n",
    "        # Spliting the words\n",
    "\n",
    "        wordfreq = []\n",
    "        for sentence in sentences:\n",
    "            wordfreq.extend(re.split(' +', sentence.lower()))\n",
    "        \n",
    "        return Counter(wordfreq)"
   ],
   "execution_count": 114,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJ1V2rv2x1Xe"
   },
   "source": [
    "### 3.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "PtpmDf3Lx1Xe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6e2ce7d8-3758-4fca-a8a6-26b4fa91bfb7"
   },
   "source": [
    "doc = HtmlDocumentTextData(\"https://innopolis.university/\")\n",
    "\n",
    "print(doc.get_word_stats().most_common(10))\n",
    "assert [x for x in doc.get_word_stats().most_common(10) if x[0] == 'иннополис'], 'иннополис sould be among most common'"
   ],
   "execution_count": 115,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "[('', 804), ('и', 46), ('в', 30), ('иннополис', 21), ('=', 14), ('на', 13), ('по', 13), ('университет', 11), ('университета', 10), ('лаборатория', 10)]\n"
     ],
     "name": "stdout"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1CIpLTFvx1Xf"
   },
   "source": [
    "## 4. Crawling ##\n",
    "\n",
    "Method `crawl_generator()` is given starting url (`source`) and max depth of search. It should return a **generator** of `HtmlDocumentTextData` objects (return a document as soon as it is downloaded and parsed). You can benefit from `yield obj_name` python construction. Use `HtmlDocumentTextData.anchors` field to go deeper."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jBrMoCjpx1Xf"
   },
   "source": [
    "from queue import Queue\n",
    "\n",
    "class Crawler:\n",
    "    \n",
    "    def crawl_generator(self, source, depth=1):\n",
    "        q = Queue()\n",
    "        q.put(source)\n",
    "\n",
    "        for i in range(depth):\n",
    "          url = q.get()\n",
    "          content = HtmlDocumentTextData(url)\n",
    "\n",
    "          for (text, url) in content.doc.anchors:\n",
    "            q.put(url)\n",
    "          \n",
    "          yield content"
   ],
   "execution_count": 129,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpSFPYCBx1Xg"
   },
   "source": [
    "### 4.1. Tests ###"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGuyqohNx1Xg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2ffe500-ad54-44b0-cea9-71433128f13f"
   },
   "source": [
    "crawler = Crawler()\n",
    "counter = Counter()\n",
    "\n",
    "for c in crawler.crawl_generator(\"https://innopolis.university/en/\", 2):\n",
    "    print(c.doc.url)\n",
    "    if c.doc.url[-4:] in ('.pdf', '.mp3', '.avi', '.mp4', '.txt'):\n",
    "        print(\"Skipping\", c.doc.url)\n",
    "        continue\n",
    "    counter.update(c.get_word_stats())\n",
    "    print(len(counter), \"distinct word(s) so far\")\n",
    "    \n",
    "print(\"Done\")\n",
    "\n",
    "print(counter.most_common(20))\n",
    "assert [x for x in counter.most_common(20) if x[0] == 'innopolis'], 'innopolis sould be among most common'"
   ],
   "execution_count": 130,
   "outputs": [
    {
     "output_type": "stream",
     "text": [
      "https://innopolis.university/en/\n",
      "485 distinct word(s) so far\n",
      "http://old.innopolis.university/en/\n",
      "1049 distinct word(s) so far\n",
      "Done\n",
      "[('', 1768), ('the', 108), ('of', 89), ('and', 87), ('=', 43), ('innopolis', 40), ('in', 37), ('to', 37), ('for', 37), ('university', 33), ('will', 24), ('research', 24), ('a', 22), ('ул.', 22), ('new', 20), ('it', 20), ('education', 20), ('international', 19), ('programs', 16), ('institute', 16)]\n"
     ],
     "name": "stdout"
    }
   ]
  }
 ]
}